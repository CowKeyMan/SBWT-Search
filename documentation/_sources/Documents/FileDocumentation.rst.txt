File Documentation
==================

This page will go through each file with a docstring and display them here in a structured order. This page is a good reference to keep open while working as a quick glance to see what each file does.


* `.clang-format`: This is the file used by the clang-format tool, which makes sure that the  code base follows a consistent coding style. For those familiar, it is  similar to python’s flake8. Inside the file we specify how we want our  codebase to look. For detailed explanation of each option, the reader is  referred to the official documentation which also contains a lot of examples  for each option: https://clang.llvm.org/docs/ClangFormatStyleOptions.html.  Rather than running clang-format directly, we make use of 2 python scripts:  scripts/cmake_checks/check_clang_format.py and  scripts/standalone/apply_clang_format.py.
* `.clang-tidy`: This is the file used by the clang-tidy tool, which performs static analysis. Inside the file we specify which files to ignore when going through the codebase. Rather than using clang-tidy directly, we make use of a python script: scripts/cmake_checks/clang_tidy.py. Note: clang-tidy may fail the first time we run scripts/build/build_all.sh since the compilation database would not have been built by then. So when we run it afterwards it will be successful.
* `.clangd`: This file indicates to the clang compiler (which is used for linting) certain options. This includes for example ignoring certain flags which are specific to the nvcc compiler
* `.gitignore`: Specifies which files should be ignored by our git repository
* `.ycm_extra_conf.py`: This file is used by the YouCompleteMe language server to get the build from the compilation_database_folder (./build in our case)
* `CMakeLists.txt`: This is the root CMakeLists which calls all other cmake files. It is used by the cmake tool to build the Makefile which would then be used to compile our program. Inside it, we specify the project name and details, we load all files within the cmake folder and then add the src and documentation folder, which also contain a CMakeLists.txt file, as subdirectories, so that the CMakeLists.txt file within them is also executed (sequentially).  Since we will be using CUDA and HIP, we want to use a pretty high version of CMake.
* **.github**:

  * **workflows**:

    * `publish-documentation.yml`: This is the file responsible for building the documentation and tests on github using github-actions and then publishing both the documentation and test results + code coverage as a github page. Within the file, each step has a name which describes its purpose.

* **benchmark_objects**: A folder to hold objects used for benchmarking. Can be downloaded using the configure scripts

  * `.gitignore`: Gitignore to ignore everything but keep the folder in git

* **cmake**: Contains cmake files which act as helper functions for cmake

  * `CCache.cmake`: Enables CCache, which prevents rebuilding of files which are already built. If not available, it is ignored
  * `CompilerWarnings.cmake`: This file creates a function to enable compiler warnings for both gcc and MSVC as well
  * `Options.cmake`: This enables or disables several CMake options, such as having Interprodecural optimisation, and positioning independent code
  * `PreventBuildingInCmakeDirectory.cmake`: This file ensures that CMake does not run in a folder where there is an existing CMakeLists.txt, as this is usually not desired. It is better to build in a separate directory as CMake generates a lot of 'garbage' files for caching
  * `Profiling.cmake`: Adds the flags -pg to gcc so we can profile with gprof
  * `SetHipTargetDevice.cmake`: Sets the target device for the build, which is either NVIDIA, AMD or CPU. Here you may also change the target architectures for the AMD target

* **docs**:

  * `.gitignore`: Ignore everything except itself and index.html

* **documentation**: Any files related to documentation contents will be kept here. Documentation is generated by both doxygen and Sphinx

  * `.gitignore`: Ignores unnecessary files and files which are generated on the fly when building
  * `CMakeLists.txt`: Looks for Doxygen and fails if it cannot find it. If it finds it, it will replace any text within the specified files (usually these files will end with the .in extension) which is surrounded by @ by the parameters provided by cmake. For example, if we look at the first line of documentation/index.rst, you will notice that part of the title is surrounded by @. This part will be replaced by the same value as it is within cmake. The output file is the same file but without the .in extension, so that the original file is unmodified. So if the input is a.txt.in, the output will be a.txt. Look at the configure_file cmake command here: https://cmake.org/cmake/help/latest/command/configure_file.html. This CMakeLists.txt is not to be used directly, but rather through the scripts/build/build_docs.sh script.
  * `Doxyfile.in`: This file describes the settings to be used by the documentation system doxygen (www.doxygen.org) for a project. Change the settings here as you see fit, however make sure to keep GENERATE_XML to YES, because this output is used by Sphinx’s breathe extension to generate our documentation.
  * `conf.py.in`: Configuration file for the Sphinx documentation builder. This is where we specify the theme, the extensions we wish to use and other options for our documentation
  * `make.bat`: This is the Windows equivalent of the Makefile for the documentation. It is usually unused.
  * **RST**:

    * `.gitignore`: Ignore everything in the folder except for itself and the .doc_string

  * **WriteUps**:

    * **Thesis**:

      * `.gitignore`: 
      * **scripts**:

        * `compile.sh`: Compiles the thesis
        * `compose_dbg.py`: A script to extract kmers from a string of raw reads and outputs the kmers as well as a De Bruijn Graph construction in the mermaid format

  * **static**:

    * `.gitignore`: Ignore everything in the folder except for itself

* **hooks**:

  * `ascii_and_whitespace.sh`: The default git pre-commit hook to remove trailing whitespaces. It also makes sure that all files are ascii
  * `pre-commit`: Used as a pre-commit check for git. Adding or removing commands is extremely easy by changing the 'hook_commands' list. If any of the commands fail, then the hook will fail.

* **scripts**: This folder contains scripts written in multiple languages which are used to combine multiple commands into a single file. It is advised to only build using these scripts rather than using the individual Tools directly.

  * **benchmark**:

    * `color_search_d1.sh`: Run the benchmark_main several times and save the results to a file. Called by scipts/sbatch/benchmark.sbatch It is expected that the \*.tdbg file is within benchmark_objects/colors folder
    * `color_search_d20.sh`: Run the benchmark_main several times and save the results to a file. Called by scipts/sbatch/benchmark.sbatch It is expected that the \*.tdbg file is within benchmark_objects/colors folder
    * `index_search_d1.sh`: Run the benchmark_main several times and save the results to a file. Called by scipts/sbatch/benchmark.sbatch It is expected that the \*.tdbg file is within benchmark_objects/index folder
    * `index_search_d20.sh`: Run the benchmark_main several times and save the results to a file. Called by scipts/sbatch/benchmark.sbatch It is expected that the \*.tdbg file is within benchmark_objects/index folder

  * **build**: Scripts used to build executables and documentation. Within this script, cmake configuration options are set. Change these if you wish to set different options.

    * `all.sh`: Build the compilation commands, main executable in release mode, tests as well as documentation. It takes a single argument which is one of NVIDIA, AMD or CPU.
    * `compilation_commands.sh`: Generate the compilation commands database for the target platform. It takes a single argument which is one of NVIDIA, AMD or CPU. The result will be found in build/compile_commands.json.
    * `debug.sh`: Build the debug version of the main executable for the target platform. It takes a single argument which is one of NVIDIA, AMD or CPU. If any other argument (any sequence of characters is accepted) is given besides these 3, it will skip the cmake step and run the build step only.
    * `docs.sh`: Build only the documentation. If any argument at all is passed to this script, it will skip the cmake step and just execute the build step only
    * `release.sh`: Build the release version of the main executable for the target platform. It takes a single argument which is one of NVIDIA, AMD or CPU. If any other argument (any sequence of characters is accepted) is given besides these 3, it will skip the cmake step and run the build step only.
    * `tests.sh`: Build the tests for the target platform. It takes a single argument which is one of NVIDIA, AMD or CPU. If any other argument (any sequence of characters is accepted) is given besides these 3, it will skip the cmake step and run the build step only.

  * **configure**:

    * `download_benchmark_data.sh`: Download the data used for benchmarking from public databases. The dataset file list is contained `scripts/configure/benchmark_datra_list.txt`. This file contains all the files obtained from https://www.ebi.ac.uk/ena/browser/view/PRJEB32631, from this paper: doi:10.1038/s41467-022-35178-5. The original dataset is 2TB big, so I have chosen to only have the first 20 files. The index contains ecoli genomes, found at https://zenodo.org/record/6656897#.ZASdeR9BwQ8, created using themisto using the genomes compiled in these papers: https://doi.org/10.1099/mgen.0.000499, https://doi.org/10.1099/mgen.0.000499 and https://doi.org/10.1038/s41586-019-1560-1
    * `download_dummy_benchmark_data.sh`: This script is used to test benchmarking locally. We download the pre made benchmark files from dropbox and unzip them locally
    * `download_large_test_objects.sh`: Download some test objects which should probably not be as part of the repository. This script makes sure to not download again if the target folder already exists. However, if any parameter at all is passed, this is ignored and the existing folder is deleted and replaced.
    * `install_hip.sh`: Helper script called in cmake/SetHipTargetDevice.cmake to automatically download the necessary hip extras alongside the already installed target device compilers. Example: the user is expected to already have nvcc installed on an NVIDIA machine, so the script then simply downloads and installs any extras related to HIP. Similarly on AMD. To install the necessary items on each platform, the user is encouraged to look at the :ref:`Tools` section of the documentation

  * **modifiers**:

    * `apply_clang_format.py`: Applies the changes proposed by clang format. Warning: This actually changes the source code. While it would not change the contents, make sure that you agree with the changes (you can check what changes will be applied if you run ./scripts/static_analysers/clang_format.py)
    * `sbwt_index_results_to_ascii.py`: Converts output from algbio/sbwt to the ascii format produced by this program
    * `themisto_colors_to_ascii.py`: Converts (sorted) output from themisto into the same output format as this repo
    * `themisto_tdbg_to_sbwt.py`: Converts \*.tdbg files to \*.sbwt files by adding 'plain-matrix' at the beginning
    * `unix_line_endings.sh`: run twice as sometimes once is not enough Only run on tracked files

  * **sbatch**:

    * `lumi_benchmark_colors_search_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on LUMI supercomputer by CSC: https://www.csc.fi/lumi
    * `lumi_benchmark_colors_search_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on LUMI supercomputer by CSC: https://www.csc.fi/lumi
    * `lumi_benchmark_index_search_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on LUMI supercomputer by CSC: https://www.csc.fi/lumi
    * `lumi_benchmark_index_search_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on LUMI supercomputer by CSC: https://www.csc.fi/lumi
    * `lumi_benchmark_themisto_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `lumi_benchmark_themisto_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `mahti_benchmark_color_search_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `mahti_benchmark_color_search_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `mahti_benchmark_index_search_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `mahti_benchmark_index_search_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `mahti_benchmark_themisto_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `mahti_benchmark_themisto_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Mahti supercomputer by CSC: https://research.csc.fi/-/mahti
    * `puhti_benchmark_color_search_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Puhti supercomputer by CSC: https://research.csc.fi/-/puhti
    * `puhti_benchmark_color_search_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Puhti supercomputer by CSC: https://research.csc.fi/-/puhti
    * `puhti_benchmark_index_search_d1.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Puhti supercomputer by CSC: https://research.csc.fi/-/puhti
    * `puhti_benchmark_index_search_d20.sbatch`: This will configure items for benchmarking and then run the benchmarks through the scripts/standalone/benchmark.sh script. Meant to be run on Puhti supercomputer by CSC: https://research.csc.fi/-/puhti

  * **standalone**: Scripts which usualy alter the repository in some manner, either by modifying files or by adding new files

    * `benchmark_analysis.py`: Script to analyse the benchmark output.
    * `generate_file_documentation.py`: Goes through the specified files in the repository, reads the first comment, and generates a documentation page file tree with the directory structure and the comment next to the file. This makes it easier to generate documentation where the user can see all the file descriptions in one documentation page

  * **static_analysers**:

    * `clang_format.py`: clang-format is a bit difficult to work with on its own. Its output is not very easy to read. Hence, here we have created a script to make it easier to work with. It creates temporary files so that we see the difference between each file individually, and then we format them as we see fit in stdout, to be easier for the eyes and to see which file is being analysed at the time  This script is executed automatically by cmake when building
    * `clang_tidy.py`: clang-tidy needs the files input manually. However, we do not want to put test files in there, as these trigger a chain reaction to add the googltest imported header files as well. Hence we simply glob for all the other .h and .cpp files and put them through the tool  This script is executed automatically by cmake when building
    * | `headers.py`: 
       Checks that the header files start with:
      |
      | #ifndef FILE_NAME_CAPITALIZED_<EXTENSION>
      | #define FILE_NAME_CAPITALIZED_<EXTENSION>
      |
      | /**
      |  * @file FileName.<extension>
      |  * @brief <description goes here and on next line>
      |
      | and ends with:
      |
      | #endif
      |
      | The term FILE_NAME_CAPITALIZED_<EXTENSION> would be replaced by the filename. So if we have a file called MyClass.h, it would be MY_CLASS_H The steps are:
      |     * Remove the .h
      |     * Convert from camel case to snake case
      |     * capitalise everything
      |     * add a _H at the end
      |
      | If the file is a cuh file, it is the same as above but it will have _CUH at the end
      |
      | The script also checks that python files start with:
      |
      | #!/usr/bin/python3
      | <new line here>
      | <3 " here, ie a docstring>
      |
      | And bash files should start with the following:
      |
      | #!/usr/bin/bash
      | <new line here>
      | # some comment

  * **test**:

    * `color_pipeline_test.sh`: Build the main executable, run the executable on test_objects/full_pipeline/color_search/ and verify that the outputs are correct (ie equal to the expected values, which can be found in the same folder)
    * `d20_pipeline_test.sh`: Build the main executable, run the executable on test_objects/full_pipeline/color_search/ and verify that the outputs are correct (ie equal to the expected values, which can be found in the same folder). The temporary files are kept in tmp/d20_pipeline_test and are subsequently delted after the test is finished.
    * `index_pipeline_test.sh`: Build the main executable, run the executable on test_objects/full_pipeline/index_pipeline_search/ and verify that the outputs are correct (ie equal to the expected values, which can be found in the same folder)
    * `test.sh`: Run the tests as well as generate code coverage. Also generates html so that code coverage can easily be seen in the docs.
    * `verify_color_results_equal.py`: Gets a list of files output from the color results, which can be of different formats, and makes sure that their contents are the same.  (usage: ./verify_files_equal.py -l <file 1> <file 2>)
    * `verify_index_results_equal.py`: Gets a list of files output from the index results, which can be of different formats, and makes sure that their contents are the same.  (usage: ./verify_files_equal.py -l <file 1> <file 2>)

* **src**:

  * `BuildCommon.cmake`: Builds items which are commonly used between the main program and the tests. Usually these are classes, files and options which are used by the main program but are also tested individually. Any common options are put as an interface rather than putting it with each file individually
  * `BuildMain.cmake`: Builds the main program, linking all the files and header files together
  * `BuildTests.cmake`: Builds the testing program. We use googletest as a testing framework
  * `BuildTools.cmake`: This file contains all the compilation necessary for modules I created to make development easier which can be used across projects not just for this one
  * `CMakeLists.txt`: Builds the main as well as testing programs, through the use of other .cmake files in this folder
  * **ArgumentParser**:

    * `ArgumentParser.h`: Parent class for argument parsers which take command line arguments and parses them into usable data structures
    * `ColorSearchArgumentParser.h`: Command line argument parser for the color searching / pseudoalignment module
    * `IndexSearchArgumentParser.h`: Contains functions to parse the main program's arguments

  * **BatchObjects**:

    * `BitSeqBatch.h`: Container for the bit sequences. These are the converted binary versions from  the string representation
    * `ColorsBatch.h`: Stores the colors contiguously for each colored sequence. A colored sequence means that the sequence has found_idxs > 0.
    * `IndexesBatch.h`: Contains the indexes of the search function which are output to disk in the 'index' phase. These are warped, meaning that the sequences are padded to the multiple of the next warp_size. Warp intervals tell us at which warp the current sequence starts and ends.
    * `IntervalBatch.h`: Data class for the intervals. The chars_before_newline is a vector which gives the number of characters that a string contains, before the new one ends. newlines_before_newfile is a vector which tells us how many newlines we need before we need to start considering the next lines or sequences as originating from a new file. Note: the last character of both vectores will always be the max value (ULLONG_MAX), and both vectors are cumulative
    * `InvalidCharsBatch.h`: Contains a binary vector of wether a string is valid or not
    * `PositionsBatch.h`: Has data created by the positions builder, which is a list of valid positions within a string sequence where the SBWT should search. A valid sequence is one where the string length is equal to the k-mer size. For example, given the string ABCDE and FGHIJ, and k-mer size 3, positions 0, 1, 2 are valid positions, but position 3, which is the k-mer starting at D, is not valid. Hence, our final position list will be: [0, 1, 2, 5, 6, 7]
    * `ResultsBatch.h`: Contains the vector with the results obtained after searching for the k-mer within the SBWT index
    * `SeqStatisticsBatch.h`: Stores statistics about each sequence of indexes. These include how many ids were actually found within this seq, how many were invalid and how many were not found. Each of these are vectors, since a single batch can have many seqs. A seq may be split between 2 batches, so we also also store a boolean if the first seq of this batch should be joined with the last seq of the previous batch. We also have a colored_seq_id, which is useful since we do not store '0' colors for warps and seqs without colors, so we keep a cumulative id here for the colors id, where the colors are stored in a separate batch object. The seqs_before_new_file stores how many sequences we need to process before we need to start considering the next sequences as part of the next file.
    * `StringBreakBatch.h`: Data which contains data about the points where strings end and another starts, as well as the string size which is how long the current character vector associated with this batch is. Note that chars_before_newline is cumulative
    * `StringSequenceBatch.h`: Data class for the string sequence associated with this batch, which is a simple pointer of a character vector

  * **ColorIndexBuilder**:

    * `ColorIndexBuilder.h`: Here, the colors file is read from disk and the CpuColorIndexContainer is constructed from it

  * **ColorIndexContainer**:

    * `CpuColorIndexContainer.h`: A container which holds items related to the color sets in the cpu
    * `GpuColorIndexContainer.h`: A container which holds items related to the color sets in the gpu

  * **ColorResultsPrinter**:

    * `AsciiContinuousColorResultsPrinter.h`: Outputs ascii results. Color indexes are ordered and space separated, and each seq is placed on a new line. When calculating memory reservations for this class, we use the max_index to see how many characters we really need per index, rather than the maximum needed for the maximum u64. This saves us a lot of space.
    * `BinaryContinuousColorResultsPrinter.h`: Outputs binary results. Color indexes are ordered and space separated, and each seq is placed on a new line
    * `ContinuousColorResultsPrinter.hpp`: Prints out the color results in parallel. Each threads handles an equal number of sequences (colored or not). Then these are first printed to a buffer in parallel, and later serially output to disk.
    * `CsvContinuousColorResultsPrinter.h`: Outputs csv results. Color indexes are ordered and space separated, and each seq is placed on a new line

  * **ColorSearcher**:

    * `ColorPostProcessor.cuh`: Squeezes the color results by adding the color sets of the same warp together and storing the results in a new array where the color results are stored contiguously. Each index handles a single color from a single sequence.
    * `ColorSearcher.cuh`: Search kernel for searching for the colors and merging them. The '*_set_bits' variables take a u64 with the least significant <width> bits set to 1, while the rest are 0s. The <width> is the width of the respective variable length vector that they correspond to.
    * `ColorSearcher.h`: Offloads color searching and memory copies to the gpu, given the indexes
    * `ContinuousColorSearcher.h`: Takes the index batch, searches for its color sets, and post processes them to give the color sums for each colored sequence. The searching and post processing are done through gpu kernel launches

  * **FilenamesParser**:

    * `FilenamesParser.h`: Takes the input and output user input and outputs if the files are a direct input or if they are a series of files. They are considered a series of files if the extension of the file is '.list'

  * **FilesizeLoadBalancer**:

    * `FilesizeLoadBalancer.h`: Takes in a list of files and a number N. N determines how many partitions (streams) we want for the files. The vector of files is then split into N buckets of approximately equal size. This is known as the multiway partitioning problem and more information about it can be found here: https://en.wikipedia.org/wiki/Multiway_number_partitioning. In our implementation, we use the greedy algorithm due to its simplicity and speed, and it comes close to a bound of 4/3 of the optimal solution. This method is also called the longest processing time first algorithm and more information can be found here: https://en.wikipedia.org/wiki/Longest-processing-time-first_scheduling. Other methods were attempted but they were too slow, especially as N increases.

  * **IndexFileParser**:

    * `AsciiIndexFileParser.h`: Index file parser for ascii files
    * `BinaryIndexFileParser.h`: Index file parser for binary files
    * `ContinuousIndexFileParser.h`: Reads a list of files one by one, filling in the batches producer as it goes along. Uses the sub IndexFileParsers to do its parsing for it. Indexes are padded to the next warp and sequence statistics are counted as well.
    * `IndexFileParser.h`: Parent template class for reading the list of integers provided by the indexing function. Provides a padded list of integers per seq and another list of indexes to indicate where each seq starts in our list of integers. Note: these classes expect the input to have the version number as the first item, and then the contents later. The format encoded in the file's header is read by another part of the code
    * `IndexFileParserTestUtils.h`: Methods used by the testing modules of the IndexFileParsers
    * `IndexesBatchProducer.h`: Simple class used by the IndexFileParser which stores the index batch and serves it to its consumers.
    * `SeqStatisticsBatchProducer.h`: Producer for SeqStatisticsBatch, which includes the count of found_idxs, not_found_idxs and invalid_idxs for each read in the current batch, as well as when each file starts and ends and other information about the sequence list.

  * **IndexResultsPrinter**:

    * `AsciiContinuousIndexResultsPrinter.h`: Inherits ContinuousIndexResultsPrinter and prints out ascii values. Indexes are printed as their space separated ASCII values, with a newline character separating the reads. Not-found values are printed as '-1' and invalid values are printed as '-2'. When calculating memory reservations for this class, we use the max_index to see how many characters we really need per index, rather than the maximum needed for the maximum u64. This saves us a lot of space.
    * `BinaryContinuousIndexResultsPrinter.h`: Inherits ContinuousIndexResultsPrinter and prints out binary values. Not-found characters are reprented with a max_u64, invalids with a max_u64-1 and newlines with a max_u64-2.
    * `BoolContinuousIndexResultsPrinter.h`: Inherits ContinuousIndexResultsPrinter and prints out values in ASCII format. If a result is found, we print out a 0, if it is not found, we print out a 1 and if it is invalid, we print 2. Different sequences are split with a newline character. This output format is not suitable for pseudoalignment since we lose the index value but it is the fastest and has the least memory footprint.
    * `ContinuousIndexResultsPrinter.hpp`: Gets results, intervals and list of invalid chars and prints the results out to disk based on the given data and filenames. This printing is done in parallel by first printing to a memory buffer and then outputting this to disk. This class uses the Template Pattern, with CRTP. Honestly its a disgusting class, mosly due to all the variables it needs to manage in order to be efficient and highly parallel. All the variable names are also super long, and then I tried to give them some short name but it still ended up seeming obscure. I tried my best to make it as easy to understand as possible, but good luck!

  * **IndexSearcher**:

    * `ContinuousIndexSearcher.h`: Search implementation with threads
    * `IndexSearcher.cuh`: Search implementation
    * `IndexSearcher.h`: Class for searching the SBWT index

  * **Main**:

    * `ColorSearchMain.h`: The main function for searching for the colors. The 'color' mode of the main executable
    * `IndexSearchMain.h`: The main function for searching the index. The 'index' mode of the main executable.
    * `Main.h`: Interface class for main classes

  * **Poppy**:

    * `Poppy.h`: The rank data structure container as discussed in the paper "Space-Efficient, High-Performance Rank & Select Structures on Uncompressed Bit Sequences" by Zhou et. al.

  * **PoppyBuilder**:

    * | `PoppyBuilder.h`: 
       Module responsible for building the rank index of the SBWT file
      | Assumptions made:
      |   * basicblock_bits is a multiple of 64
      |   * superblock_bits is a multiple of basicblock_bits
      |   * hyperblock_bits is a multiple of hyperblock_bits

  * **PositionsBuilder**:

    * `ContinuousPositionsBuilder.h`: Builds the positions of the indexes of the first characters of the kmers in our sequences in the batch
    * `PositionsBuilder.h`: Builds a vector of the positions of the starting chracter of each kmer in our sequence

  * **Presearcher**:

    * `Presearcher.cuh`: Device function for presearching
    * `Presearcher.h`: The presearcher will search for all permutations of k-mers of a certain size and cache them, so that future searches can continue from this checkpoint

  * **SbwtBuilder**:

    * `SbwtBuilder.h`: Loads SBWT from disk and also builds the other components such as their Poppy data structure and the c-map. If prompted, it will also load the key-kmer marks from disk from the colors file. These components are stored in the CpuSbwtContainer.

  * **SbwtContainer**:

    * `CpuSbwtContainer.h`: SbwtContainer for that on the cpu side. Contains the acgt bitvectors, their Poppys, the c-map and also possibly the key-kmer marks, if loaded.
    * `GpuSbwtContainer.h`: Contains the same items as the CpuSbwtContainer but as pointers on the GPU
    * `SbwtContainer.h`: Contains data class for SBWT index. Contains the more generic items from the SBWT items such as the bit vector sizes, and the kmer size. Cpu and Gpu specific items are in the subclasses.

  * **SeqToBitsConverter**:

    * `BitsProducer.h`: Transforms a list of ACTG characters into their 2-bit equivalent and packs them into a u64 bitvector
    * `CharToBits.h`: Contains mapping functions which map the characters ACGT to their corresponding bit value
    * `ContinuousSeqToBitsConverter.h`: Class for converting char sequences continuously, with parallel capabilities. Also builds the invalid characters list in the same pass
    * `InvalidCharsProducer.h`: Produces a list of booleans which tell wether a character is valid or not. Instead of bool we use a 1 or 0 character since it is faster to process, even though the memory footprint is higher

  * **SequenceFileParser**:

    * `ContinuousSequenceFileParser.h`: Continuously reads sequences from a file or multiple files into a buffer. Then it can serve these sequences to its consumers. The reading is done in such a way that a single batch can contain characters from multiple lines. kseqpp_REad is used for parsing the files and getting the list of where each line break is.
    * `IntervalBatchProducer.h`: Builds the IntervalBatch, which tells where one sequence ends and another begins, and how many sequences are there before we need to start a new file.
    * `StringBreakBatchProducer.h`: In charge of storing and handing out the locations where one sequence ends and another begins, stored in the StringBreakBatch
    * `StringSequenceBatchProducer.h`: Takes care of building and producing the stringsequencebatch. This is handed over as a pointer from the ContinuousSequenceFileParser, to be shared with the next consumer component

  * **Tools**:

    * `BenchmarkUtils.hpp`: A collection of utility scripts used for benchmarking
    * `BitDefinitions.h`: Contains definitions for some useful commonly used bit related items.
    * `CircularBuffer.hpp`: Implementation of a Circular Buffer which is used for optimising memory when using large buffers
    * `CircularQueue.hpp`: Implementation of a Circular Queue which is used for optimising memory when using queues
    * `DebugUtils.hpp`: Utilities for debugging code
    * `DummyBatchProducer.hpp`: Inheriting from the SharedBatchesProducer, this class sequentially produces the batches it was given in its constructor. Useful for testing
    * `ErrorUtils.h`: Helper class for error related functionality
    * `GpuEvent.h`: A class to manage gpu events, useful for not having to create and delete events, and instead we can create them and use them as is.
    * `GpuPointer.h`: Class to abstract copying to and from the GPU
    * `GpuStream.h`: A class to manage gpu streams, useful for not having to create and delete streams, and instead we can create them and use them as is.
    * `GpuUtils.h`: Contains GPU commonly used functions and tools
    * `IOUtils.h`: Contains utilities to ease interacting with IO streams        such as check if a file exists
    * `KernelUtils.cuh`: Contains kernel device functions used commonly in some kernels
    * `Logger.h`: Utilities for structured logging
    * `MathUtils.hpp`: A collection of utility scripts to extend math functionality
    * `MemoryUnitsParser.h`: Given a string such as "1KB" or "1000" (bits) or "10 GB", this module converts it to bits.
    * `MemoryUtils.h`: Utilities which deal with memory management and querying
    * `OmpLock.h`: A wrapper over omp_lock_t to allow deletion when the resource is released, like a smart pointer
    * `PinnedVector.h`: This class is part of the gpu utilities and represents an array of fixed size but with some nice utilities to interact with other gpu utilities. It uses pinned memory so that memory transfers between cpu and gpu are much faster.
    * `RNGUtils.hpp`: Utilities for random number generation
    * `Semaphore.h`: A C++ implementation for counted semaphores using standard library mutex and condition variables. Credit for class base: http://www.cs.umd.edu/~shankar/412-Notes/10x-countingSemUsingBinarySem.pdf
    * `SharedBatchesProducer.hpp`: Template class for any class which is a continuous batch producer that shares its batch
    * `StdUtils.hpp`: Functions to help out with standard library items
    * `TestUtils.hpp`: Contains functions to make testing functions cleaner
    * `TypeDefinitions.h`: Contains type definitions created for convenience

  * **UtilityKernels**:

    * `GetBoolFromBitVector.cuh`: Given a bit vector (which is an array of u64s, where each bit in each element should be considered individually), this function will get the index of a certain bit within the whole bit vector and return wether it is False (0) or True (1). For example, given the index 78, this will be found on the 2nd element of the u64 array, and we must check the (78 - 64 =) 14th bit. This is the same as the sdsl-lite library's bit vector.
    * `GetBoolFromBitVector_test.cuh`: A simple kernel which performs the GetBoolFromBitVector function for a single item in the gpu. Used only for testing
    * `GetBoolFromBitVector_test.h`: Header for function used in testing the GetBoolFromBitVector kernel
    * `Rank.cuh`: The rank function is given an index within a bit vector and returns the number of 1s up to that point. We use the poppy data structure which is described in the paper Space-Efficient, High-Performance Rank & Select Structures on Uncompressed Bit Sequences by Zhou et. al. This data structure is also included in the sdsl library with rank_support_v5 (see test for this class if you wish to know how that is used).
    * `Rank_test.cuh`: A simple kernel which performs rank function for a single item in the gpu. Used only for testing
    * `Rank_test.h`: Header for functions used in testing the device rank function
    * `VariableLengthIntegerIndex.cuh`: Given a u64 integer vector whose size is not a regular number such as 32 or 64 bits, we can index it using this function. So for example, given a vector of u64s which contain the bits for a vector of integers of 12 bits each, we can get the 1st, 2nd,...,nth element using this function. Note: only works for variables with number of bits less than or equal to the bits of the container (so if we have a list of u64 elements, the bit size of the containing elements must be less than 64. This kernel is usually used for indexing sdsl int_vectors of irregular size. The same implementation is done by sdsl: https://github.com/simongog/sdsl-lite/blob/master/include/sdsl/bits.hpp#L501
    * `VariableLengthIntegerIndex_test.cuh`: Test kernel for VariableLengthIntegerIndex
    * `VariableLengthIntegerIndex_test.h`: Header for functions used in testing the d_variable_length_int_index

* **test_objects**: Contains objects used for testing, such as small input files which are used by the various unit tests

  * **full_pipeline**:

    * **index_search**: Here are some files which are used by tests/standalone/pipeline_test.sh to test our outputs and thus making sure the index search program works end to end

  * **tmp**:

    * `.gitignore`: 

